# Market Sentinel

Plataforma async-first de anal√≠tica para App Stores e inteligencia competitiva. Ingesta, normaliza y analiza datos (precios, reviews, changelogs) de marketplaces de aplicaciones.

## Tech Stack

- **Python 3.12+** (async-first, fully typed)
- **FastAPI** + Uvicorn (async REST API)
- **PostgreSQL 16** via async SQLAlchemy 2.0 + asyncpg
- **Redis 7** + Arq (async background task queue)
- **Polars** (data processing)
- **Alembic** (migraciones de base de datos async)
- **Docker Compose** (postgres, redis, app)
- **Poetry** (dependency management)

## Comandos de Ejecuci√≥n

```bash
# Instalar dependencias
poetry install

# Levantar todos los servicios (postgres, redis, app en :8000)
docker compose up --build

# Ejecutar localmente (requiere postgres/redis corriendo por separado)
poetry run uvicorn src.api:app --reload --host 0.0.0.0 --port 8000
```

## Migraciones de Base de Datos (Alembic)

Alembic gestiona la evoluci√≥n del esquema de PostgreSQL de forma versionada y reproducible. Est√° configurado para trabajar de forma **as√≠ncrona** con `asyncpg`.

```bash
# Aplicar todas las migraciones pendientes
poetry run alembic upgrade head

# Revertir la √∫ltima migraci√≥n
poetry run alembic downgrade -1

# Generar nueva migraci√≥n autom√°tica tras cambiar modelos
poetry run alembic revision --autogenerate -m "descripci√≥n del cambio"

# Ver la migraci√≥n actual
poetry run alembic current

# Ver historial de migraciones
poetry run alembic history
```

### Esquema actual (`001_initial_schema`)

- **`apps`** ‚Äî Cat√°logo de aplicaciones con constraint √∫nico `(bundle_id, store)`
- **`price_history`** ‚Äî Serie temporal de precios, particionada por mes (2026-2028) con √≠ndices BRIN
- **`reviews`** ‚Äî Rese√±as con columna JSONB `metadata` e √≠ndice GIN

## Tests

```bash
# Ejecutar todos los tests
poetry run pytest

# Ejecutar un archivo de test espec√≠fico
poetry run pytest tests/test_example.py

# Ejecutar con output detallado
poetry run pytest -v

# Ejecutar tests que coincidan con un patr√≥n
poetry run pytest -k "test_nombre"
```

## Lint y Formato

```bash
# Verificar y corregir estilo
ruff check src/ --fix

# Formatear c√≥digo
ruff format src/

# Pre-commit hooks (instalar una vez)
pre-commit install
pre-commit run --all-files
```

## Arquitectura

```
src/
‚îú‚îÄ‚îÄ api/          # FastAPI app, REST endpoints
‚îú‚îÄ‚îÄ core/         # Config (Pydantic Settings), database engine
‚îú‚îÄ‚îÄ modules/      # Dominios de negocio (apps, scraping, analytics)
‚îî‚îÄ‚îÄ worker/       # Arq background jobs
```

---

# **`Programa de Alto Rendimiento: AI & AppSec (24 Semanas)`**

- ProgramaDominio de SQL y Python para Datos: Es la base de todo lo que sigue.
- RAG y Orquestaci√≥n (LangChain): Para entrar en la ola de la IA.
- Docker y Cloud (AWS): Para que dejes de ser un "programador de PC" y seas un "Ingeniero de Sistemas".
- Especializaci√≥n en Seguridad (AppSec): A largo plazo, para blindar tu carrera.

## Fase 1: Data Engineering & Advanced Backend (Semanas 1-6)

**Objetivo:** Dominar la ingesta masiva de datos. El **DOMINIO DE SQL Y PYTHON PARA DATOS** es lo que separa a un programador de un ingeniero de sistemas.

- **Deep-Dive:** Optimizaci√≥n de planes de ejecuci√≥n en Postgres, particionamiento de tablas para series temporales (logs) y el modelo de memoria de Polars.
- **Seguridad:** Implementaci√≥n de Row Level Security (RLS) en PostgreSQL y prevenci√≥n de Inyecci√≥n SQL en consultas din√°micas complejas.

| Sem | Tema Principal | Objetivo T√©cnico |
|---:|---|---|
| 1 | **PostgreSQL Internals** | Tuning de `shared_buffers`, √≠ndices GIN para JSONB y `EXPLAIN ANALYZE`. |
| 2 | **Async SQLAlchemy 2.0** | Patrones avanzados de concurrencia y gesti√≥n de pools en entornos `async`. |
| 3 | **Polars & Data Processing** | Sustituci√≥n de Pandas. Procesamiento Lazy para datasets que no entran en RAM. |
| 4 | **Vector Databases (Foundations)** | Instalaci√≥n de `pgvector` y comprensi√≥n de tipos de indexaci√≥n (IVFFlat vs HNSW). |
| 5 | **Validation & Data Integrity** | Uso avanzado de Pydantic v2 para tipado estricto en pipelines de datos. |
| 6 | **High Performance APIs** | Benchmarking de FastAPI vs Litestar para servicios de datos intensivos. |

**Capstone Project (Parte I):** Un motor de ingesta as√≠ncrono que procesa logs de red, los normaliza con Polars y los indexa vectorialmente en PostgreSQL.

---

## Fase 2: Generative AI Architecture & RAG (Semanas 7-12)

**Objetivo:** Dejar de usar "wrappers" y construir arquitecturas. Aplicaremos **RAG Y ORQUESTACI√ìN (LANGCHAIN)** para crear sistemas con memoria y contexto.

- **Deep-Dive:** Estrategias de Chunking (Semantic vs Recursive), arquitecturas Multi-Agent y manejo de estados persistentes.
- **Seguridad:** Mitigaci√≥n de fugas de datos en el contexto (Data Leakage) y sanitizaci√≥n de inputs para el LLM.

| Sem | Tema Principal | Objetivo T√©cnico |
|---:|---|---|
| 7 | **Embeddings & Search** | Comparativa de modelos (OpenAI, HuggingFace, Cohere) y m√©tricas de distancia. |
| 8 | **RAG Avanzado** | Implementaci√≥n de *Self-Querying* y *Small-to-Big Retrieval*. |
| 9 | **Agentic Workflows** | Uso de LangGraph para flujos c√≠clicos y toma de decisiones aut√≥noma. |
| 10 | **Memory & Context** | Implementaci√≥n de memoria de largo plazo usando Redis como vector store. |
| 11 | **Herramientas de IA (CLI)** | Integraci√≥n profunda de Claude Code y herramientas de terminal para desarrollo. |
| 12 | **Evaluaci√≥n de Modelos** | Frameworks de testing (DeepEval/Ragas) para medir alucinaciones. |

---

## Fase 3: MLOps & Cloud Infrastructure (Semanas 13-18)

**Objetivo:** Desplegar a escala. Usar√°s **DOCKER Y CLOUD (AWS)** para que tus modelos sean productivos y resilientes.

- **Deep-Dive:** Orquestaci√≥n de contenedores para inferencia, auto-scaling basado en latencia y despliegue de modelos *Open Source*.
- **Seguridad:** Hardening de im√°genes Docker (Distroless), escaneo de vulnerabilidades en el CI/CD y gesti√≥n de secretos en AWS.

| Sem | Tema Principal | Objetivo T√©cnico |
|---:|---|---|
| 13 | **Docker for AI** | Optimizaci√≥n de im√°genes para GPU y reducci√≥n de peso de capas. |
| 14 | **AWS Bedrock & SageMaker** | Consumo de modelos Serverless vs despliegue de clusters propios. |
| 15 | **Infrastructure as Code** | Terraform para levantar toda la infraestructura de IA de forma reproducible. |
| 16 | **CI/CD Pipelines** | GitHub Actions para tests autom√°ticos de modelos y despliegue continuo. |
| 17 | **Observabilidad** | Centralizaci√≥n de logs (CloudWatch) y trazabilidad de prompts (LangSmith). |
| 18 | **Cost Optimization** | Estrategias de Spot Instances y caching sem√°ntico para bajar costos. |

---

## Fase 4: Offensive & Defensive AI Security - AppSec (Semanas 19-24)

**Objetivo:** Tu especializaci√≥n final. La **ESPECIALIZACI√ìN EN SEGURIDAD (APPSEC)** te diferenciar√° en el mercado.

- **Deep-Dive:** OWASP Top 10 para LLMs, ataques de Inyecci√≥n de Prompts y seguridad en la cadena de suministro.
- **Seguridad:** Creaci√≥n de "Guardrails" para interceptar respuestas maliciosas en tiempo real.

| Sem | Tema Principal | Objetivo T√©cnico |
|---:|---|---|
| 19 | **LLM Attack Vectors** | Pr√°cticas de Prompt Injection (Directa e Indirecta). |
| 20 | **Insecure Output Handling** | C√≥mo evitar que un LLM ejecute c√≥digo malicioso en tu servidor (RCE). |
| 21 | **Guardrails & Filtering** | Implementaci√≥n de NeMo Guardrails para control de flujo y seguridad. |
| 22 | **Supply Chain Security** | An√°lisis de vulnerabilidades en librer√≠as de IA y modelos de HuggingFace. |
| 23 | **AI Red Teaming** | Simulaci√≥n de ataques controlados contra tu propio sistema. |
| 24 | **Hardening Final** | Auditor√≠a de punta a punta y entrega del proyecto final. |


![Diagrama](diagram-app-market-sentinel.png)

üîç An√°lisis de la Arquitectura: Flujo de Datos E2E
La imagen superior detalla la implementaci√≥n de App Market Sentinel, un sistema dise√±ado para la resiliencia y el procesamiento masivo de datos (High-Throughput).

üìÇ Estructura del Repositorio (Panel Lateral)
El proyecto adopta un patr√≥n de Monolito Modular dentro de la carpeta src/.

src/api/: Expone la l√≥gica de negocio mediante FastAPI.

src/worker/: Ejecuta las tareas pesadas de scraping de forma as√≠ncrona mediante Arq.

src/modules/: El n√∫cleo de la inteligencia, donde scraping/ extrae los datos y processor/ los limpia con Polars.

src/core/: Configuraci√≥n centralizada y observabilidad.

‚öôÔ∏è Ciclo de Vida de una Petici√≥n (Flujo Principal)
Ingesta Inteligente (Bloque Verde): A diferencia de un scraper lineal, el Async Worker utiliza HTTPX con rotaci√≥n din√°mica de User-Agents para evitar bloqueos. La librer√≠a Tenacity asegura que, ante fallos de red, el sistema reintente la operaci√≥n de forma exponencial, garantizando un 99.9% de √©xito en la captura.

Validaci√≥n y Refinado: Antes de tocar la base de datos, cada registro es validado por Pydantic v2. Luego, un pipeline de Polars (escrito en Rust) procesa los textos, elimina informaci√≥n sensible (PII) y normaliza las monedas en milisegundos.

Persistencia Avanzada (Bloque Azul): Los datos se almacenan en PostgreSQL 16 usando t√©cnicas de ingenier√≠a de alto nivel:

Particionamiento: El historial de precios se fragmenta por fechas para mantener consultas r√°pidas tras a√±os de datos.

JSONB & GIN: Las reviews se guardan como documentos flexibles pero indexados para b√∫squedas instant√°neas.

pgvector: Los datos quedan listos para b√∫squeda sem√°ntica e IA.

Consumo de Alta Performance (Bloque Naranja): La API de FastAPI no consulta las tablas pesadas directamente; lee de Vistas Materializadas pre-calculadas, devolviendo respuestas en menos de 200ms. Todo el flujo est√° protegido por autenticaci√≥n JWT y documentado autom√°ticamente bajo el est√°ndar OpenAPI.
