# Informe de Arquitectura: Motor de Ingesta de Datos (M√≥dulo de Scraping)

## üìÑ Resumen del Flujo de Trabajo

**Componentes Involucrados:**
- `src/api/__init__.py` ‚Äî Endpoint REST
- `src/worker/__init__.py` ‚Äî Configuraci√≥n del worker Arq
- `src/worker/tasks.py` ‚Äî Tareas as√≠ncronas de scraping
- `src/modules/scraping/client.py` ‚Äî Cliente HTTP con retry/rate limiting
- `src/modules/scraping/base.py` ‚Äî Scraper base abstracto
- `src/modules/scraping/stores/apple.py` ‚Äî Implementaci√≥n Apple App Store
- `src/modules/scraping/stores/google.py` ‚Äî Implementaci√≥n Google Play Store
- `src/modules/scraping/parsers.py` ‚Äî Parsers HTML especializados
- `src/modules/scraping/normalizers.py` ‚Äî Funciones puras de normalizaci√≥n
- `src/modules/scraping/schemas.py` ‚Äî Schemas Pydantic con validadores
- `src/modules/apps/models.py` ‚Äî Modelos de datos SQLAlchemy
- `src/core/database.py` ‚Äî Infraestructura de base de datos

**Prop√≥sito Integral:**

El m√≥dulo de scraping es el motor de ingesta de datos de Market Sentinel. Su misi√≥n es extraer informaci√≥n estructurada de app stores (Apple App Store, Google Play Store) y almacenarla de forma confiable en PostgreSQL. El proceso completo implementa un patr√≥n productor-consumidor as√≠ncrono donde:

1. **El API REST produce trabajos** que se encolan en Redis
2. **El worker Arq consume los trabajos** y ejecuta el scraping real
3. **Los scrapers obtienen HTML crudo** usando un cliente HTTP resiliente
4. **Los parsers extraen datos estructurados** del HTML
5. **El Data Quality Firewall normaliza y valida** antes de persistir
6. **Los datos validados se persisten** en PostgreSQL con integridad referencial

Este dise√±o desacopla la recepci√≥n de solicitudes HTTP de las operaciones de I/O costosas (HTTP requests + parsing + DB writes), permitiendo que la API responda inmediatamente mientras el trabajo pesado ocurre en background.

---

## üèó Arquitectura y Sincron√≠a

### Patr√≥n Productor-Consumidor con Redis como Message Broker

El m√≥dulo implementa una **arquitectura de colas as√≠ncronas** donde:

- **Productor (API)**: El endpoint `POST /scrape/{app_id}` recibe solicitudes HTTP, valida que el `app_id` sea v√°lido, y encola un job `scrape_app_task` en Redis usando Arq. La API responde inmediatamente con un `job_id`, sin esperar a que termine el scraping.

- **Cola (Redis)**: Act√∫a como buffer y broker de mensajes. Desacopla temporalmente el API del worker. Si el worker est√° ocupado o ca√≠do, los jobs se acumulan en Redis y se procesan cuando haya capacidad.

- **Consumidor (Worker)**: Un proceso separado (worker Arq) ejecuta un loop infinito consumiendo jobs de Redis. Por cada job, ejecuta la tarea `scrape_app_task` que:
  1. Busca la app en la base de datos
  2. Instancia el scraper apropiado (Apple/Google)
  3. Ejecuta el scraping HTTP
  4. Parsea el HTML
  5. Valida y normaliza los datos
  6. Persiste en PostgreSQL

### ¬øPor qu√© esta arquitectura?

**Resiliencia operacional**: Si el scraping tarda 30 segundos, la API no bloquea. Si hay un pico de tr√°fico, Redis act√∫a como amortiguador.

**Escalabilidad horizontal**: Se pueden lanzar m√∫ltiples workers en paralelo consumiendo de la misma cola. Cada worker procesa hasta 10 jobs concurrentes (`max_jobs = 10`).

**Aislamiento de fallos**: Un crash en el scraper no tumba el API. Los jobs fallidos pueden reintentar o moverse a una dead-letter queue.

**Control de concurrencia**: El worker usa un sem√°foro (`asyncio.Semaphore`) para limitar la concurrencia interna (ej. no hacer 1000 requests simult√°neos a Apple).

---

## üîÑ Diagrama de Interacci√≥n

```mermaid
sequenceDiagram
    autonumber
    participant Client as Cliente HTTP
    participant API as FastAPI<br/>(api/__init__.py)
    participant Redis as Redis Queue
    participant Worker as Arq Worker<br/>(worker/tasks.py)
    participant DB as PostgreSQL
    participant HTTP as HTTPClient<br/>(client.py)
    participant Scraper as AppleStoreScraper<br/>(stores/apple.py)
    participant Parser as Parsers<br/>(parsers.py)
    participant Schema as Schemas<br/>(schemas.py)

    Client->>API: POST /scrape/{app_id}
    API->>Redis: enqueue_job("scrape_app_task", app_id)
    Redis-->>API: job_id
    API-->>Client: 200 OK {"job_id": "..."}

    Note over Worker,Redis: Worker polling loop
    Worker->>Redis: fetch next job
    Redis-->>Worker: scrape_app_task(app_id)

    Worker->>DB: SELECT * FROM apps WHERE id = app_id
    DB-->>Worker: App record (bundle_id, store)

    Worker->>Scraper: scrape(bundle_id)
    Scraper->>HTTP: GET https://apps.apple.com/us/app/id{bundle_id}

    Note over HTTP: Retry logic + rate limiting
    HTTP-->>Scraper: HTML response

    Scraper->>Parser: parse_app_metadata(html)
    Parser-->>Scraper: ScrapedApp

    Scraper->>Parser: parse_price(html)
    Parser-->>Scraper: ScrapedPrice

    Scraper->>Parser: parse_reviews(html)
    Parser-->>Scraper: List[ScrapedReview]

    Scraper->>Schema: Validate ScrapedApp
    Schema->>Schema: Apply normalizers<br/>(strip, max_length, timezone)
    Schema-->>Scraper: Validated ScrapedApp

    Scraper->>Schema: Validate ScrapedPrice
    Schema->>Schema: normalize_currency<br/>ensure_timezone_aware
    Schema-->>Scraper: Validated ScrapedPrice

    Scraper-->>Worker: ScrapeResult{app, price, reviews}

    Worker->>DB: UPDATE apps SET name=..., developer_name=...
    Worker->>DB: INSERT INTO price_history (...)
    Worker->>DB: INSERT INTO reviews (...) ON CONFLICT DO NOTHING
    Worker->>DB: COMMIT
    DB-->>Worker: OK

    Worker->>Redis: mark job as complete
```

---

## üß± Componentes Clave y Responsabilidades

### 1. API REST ‚Äî Productor de Trabajos

**Archivo**: `src/api/__init__.py`

**Responsabilidades**:
- Exponer endpoint `POST /scrape/{app_id}`
- Validar que `app_id` sea un UUID v√°lido
- Encolar job en Redis usando conexi√≥n Arq
- Retornar `job_id` inmediatamente (fire-and-forget)

**Decisi√≥n de dise√±o**: La API no espera a que termine el scraping. Esto evita timeouts HTTP y permite manejar solicitudes de larga duraci√≥n. El cliente puede polling el estado del job usando el `job_id` (endpoint no implementado a√∫n, pero es el patr√≥n est√°ndar).

**C√≥digo clave**:
```python
@app.post("/scrape/{app_id}")
async def scrape_app(app_id: str) -> dict[str, str]:
    pool = await _get_arq_pool()  # Conexi√≥n lazy a Redis
    job = await pool.enqueue_job("scrape_app_task", app_id)
    if job is None:  # Job ya encolado (deduplicaci√≥n)
        raise HTTPException(status_code=409, detail="Job already enqueued")
    return {"job_id": job.job_id}
```

---

### 2. Worker Arq ‚Äî Consumidor y Orquestador

**Archivos**:
- `src/worker/__init__.py` ‚Äî Configuraci√≥n del worker
- `src/worker/tasks.py` ‚Äî Implementaci√≥n de tareas

**Responsabilidades**:
- Consumir jobs de Redis en un loop infinito
- Buscar la app en PostgreSQL para obtener `bundle_id` y `store`
- Instanciar el scraper correcto (Apple/Google)
- Ejecutar el scraping y persistir resultados
- Manejar errores y logging estructurado

**Configuraci√≥n del worker**:
```python
class WorkerSettings:
    functions = [scrape_app_task, scrape_batch_task]
    on_startup = startup  # Hook para inicializaci√≥n
    on_shutdown = shutdown  # Hook para limpieza
    redis_settings = _redis_settings()  # Conexi√≥n a Redis
    max_jobs = 10  # Concurrencia m√°xima
    job_timeout = 300  # Timeout de 5 minutos por job
```

**Flujo de `scrape_app_task`**:
1. Recibe `app_id` como par√°metro
2. Crea una sesi√≥n de base de datos as√≠ncrona
3. Busca la app: `SELECT * FROM apps WHERE id = app_id`
4. Si no existe, retorna error
5. Instancia `HTTPClient()` como context manager (cierra conexiones al terminar)
6. Obtiene el scraper usando factory: `_get_scraper(app.store, client)`
7. Ejecuta `scraper.scrape(app.bundle_id)`
8. Llama a `_save_scrape_result(session, app, scrape_result)` para persistir
9. Hace commit de la transacci√≥n
10. Retorna diccionario con `{"success": bool, "error": str | None}`

**Decisi√≥n de dise√±o**: El worker abre una sesi√≥n de DB por job (no pooling compartido) para evitar race conditions en asyncpg. Cada job es una transacci√≥n independiente.

---

### 3. HTTPClient ‚Äî Cliente HTTP Resiliente

**Archivo**: `src/modules/scraping/client.py`

**Responsabilidades**:
- Hacer requests HTTP GET as√≠ncronos
- Rotar User-Agent usando `fake-useragent` (evita detecci√≥n de bots)
- Implementar retry autom√°tico con backoff exponencial
- Detectar rate limiting (HTTP 429) y reintentar despu√©s del `Retry-After`
- Logging estructurado de cada request

**Caracter√≠sticas t√©cnicas**:
- Usa `httpx.AsyncClient` con timeout configurable (default: 30s)
- Context manager: `async with HTTPClient() as client:`
- Decorator `@retry` de `tenacity` para reintentos
- Reintentos solo en: `httpx.TimeoutException` y `RateLimitError`
- Backoff exponencial: min 1s, max 60s (configurables)

**C√≥digo clave**:
```python
async def _get_with_retry(self, url: str, *, max_retries: int, min_wait: float, max_wait: float):
    @retry(
        stop=stop_after_attempt(max_retries),
        wait=wait_exponential(min=min_wait, max=max_wait),
        retry=retry_if_exception_type((httpx.TimeoutException, RateLimitError)),
        reraise=True,
    )
    async def _do_request() -> httpx.Response:
        headers = self._build_headers()  # User-Agent random
        response = await self._client.get(url, headers=headers)

        if response.status_code == 429:
            retry_after = response.headers.get("Retry-After")
            raise RateLimitError(retry_after=float(retry_after) if retry_after else None)

        response.raise_for_status()
        return response

    return await _do_request()
```

**Decisi√≥n de dise√±o**: No reintentar en HTTP 4xx (excepto 429) porque son errores del cliente (app no existe, URL inv√°lida). Solo reintentar en errores transitorios.

---

### 4. BaseScraper ‚Äî Scraper Abstracto

**Archivo**: `src/modules/scraping/base.py`

**Responsabilidades**:
- Definir interfaz com√∫n para todos los scrapers (`scrape`, `build_url`)
- Implementar m√©todo `scrape_batch` para scraping concurrente con sem√°foro
- Manejar excepciones y convertirlas en `ScrapeResult` con `success=False`

**Patr√≥n de dise√±o**: Template Method. La clase base define el flujo (`_safe_scrape` ‚Üí `scrape`) y las subclases implementan los detalles (`build_url`, `scrape`).

**C√≥digo clave**:
```python
async def scrape_batch(self, bundle_ids: list[str], *, concurrency: int | None = None):
    limit = concurrency or get_settings().scraper_concurrency
    semaphore = asyncio.Semaphore(limit)  # Control de concurrencia

    async def _limited(bid: str) -> ScrapeResult:
        async with semaphore:
            return await self._safe_scrape(bid)

    # Ejecuta todos en paralelo, pero respetando el sem√°foro
    results = await asyncio.gather(
        *[_limited(bid) for bid in bundle_ids],
        return_exceptions=True,
    )

    # Convierte excepciones en ScrapeResult con success=False
    final: list[ScrapeResult] = []
    for i, result in enumerate(results):
        if isinstance(result, BaseException):
            final.append(ScrapeResult(url=self.build_url(bundle_ids[i]), success=False, error=str(result)))
        else:
            final.append(result)
    return final
```

**Decisi√≥n de dise√±o**: `scrape_batch` nunca lanza excepciones. Siempre retorna una lista de `ScrapeResult`, donde cada resultado puede ser exitoso o fallido. Esto permite que un error en una app no detenga el scraping de las dem√°s.

---

### 5. AppleStoreScraper ‚Äî Implementaci√≥n Apple App Store

**Archivo**: `src/modules/scraping/stores/apple.py`

**Responsabilidades**:
- Construir URL de Apple App Store: `https://apps.apple.com/us/app/id{bundle_id}`
- Hacer request HTTP usando `HTTPClient`
- Parsear HTML usando 3 parsers especializados:
  - `AppMetadataParser` ‚Üí nombre, developer, descripci√≥n, √≠cono
  - `PriceParser` ‚Üí precio, moneda, regi√≥n
  - `ReviewParser` ‚Üí reviews con rating, t√≠tulo, contenido, autor, fecha
- Retornar `ScrapeResult` con todos los datos extra√≠dos

**Parsers configurados con CSS selectors**:
```python
_APP_METADATA_PARSER = AppMetadataParser(
    name_selector="h1.product-header__title",
    developer_selector="h2.product-header__identity a",
    description_selector="section.section--description .we-truncate p",
    icon_selector="picture.product-hero__media source",
)

_PRICE_PARSER = PriceParser(
    price_selector="li.inline-list__item--bulleted:first-child",
)

_REVIEW_PARSER = ReviewParser(
    container_selector=".we-customer-review",
    id_attr="data-review-id",
    rating_selector="figure.we-star-rating",
    title_selector="h3.we-customer-review__title",
    content_selector="p.we-customer-review__body",
    author_selector="span.we-customer-review__user",
    date_selector="time.we-customer-review__date",
)
```

**Decisi√≥n de dise√±o**: Los parsers se instancian una sola vez como constantes de m√≥dulo (singleton). Son stateless y reutilizables. Los CSS selectors est√°n hardcodeados porque cada store tiene un DOM estable (si cambia, falla el test de integraci√≥n y se actualiza).

---

### 6. GooglePlayScraper ‚Äî Implementaci√≥n Google Play Store

**Archivo**: `src/modules/scraping/stores/google.py`

**Estado actual**: Stub. Solo define `build_url` y lanza `NotImplementedError` en `scrape()`.

**URL generada**: `https://play.google.com/store/apps/details?id={bundle_id}&hl=en&gl=US`

**Pendiente**: Implementar parsers HTML espec√≠ficos de Google Play. La estructura es similar a Apple pero con selectores CSS distintos.

---

### 7. Parsers HTML ‚Äî Extracci√≥n de Datos Estructurados

**Archivo**: `src/modules/scraping/parsers.py`

**Responsabilidades**:
- Parsear HTML usando BeautifulSoup (`html.parser`)
- Extraer texto y atributos usando CSS selectors
- Convertir strings crudos en tipos tipados (Decimal, int, datetime)
- Retornar Pydantic schemas (`ScrapedApp`, `ScrapedPrice`, `ScrapedReview`)

#### 7.1 HTMLParser ‚Äî Base Gen√©rica

Clase base con m√©todos de utilidad:
- `parse_html(html: str) -> BeautifulSoup`
- `extract_text(element: Tag | None) -> str | None`
- `extract_attr(element: Tag | None, attr: str) -> str | None`

#### 7.2 AppMetadataParser

Extrae informaci√≥n b√°sica de la app:
- Nombre (requerido)
- Developer
- Descripci√≥n
- URL del √≠cono

**C√≥digo clave**:
```python
def parse(self, html: str, bundle_id: str) -> ScrapedApp | None:
    soup = self.parse_html(html)
    name = self.extract_text(soup.select_one(self.name_selector))
    if not name:
        return None  # Sin nombre, la app no existe o el selector fall√≥

    return ScrapedApp(
        name=name,
        bundle_id=bundle_id,
        developer_name=self.extract_text(soup.select_one(self.developer_selector)),
        description=self.extract_text(soup.select_one(self.description_selector)),
        icon_url=self.extract_attr(soup.select_one(self.icon_selector), "src"),
    )
```

**Decisi√≥n de dise√±o**: Si el `name` no se encuentra, retorna `None` (la app no existe o el HTML cambi√≥). Los campos opcionales se pasan como `None` y Pydantic los valida.

#### 7.3 PriceParser

Extrae precio de la app:
- Detecta "Free" y lo convierte a `Decimal("0.00")`
- Limpia s√≠mbolos de moneda: `$4.99` ‚Üí `4.99`
- Convierte a `Decimal` (precisi√≥n exacta, no float)
- Asigna moneda y regi√≥n (pasadas como par√°metros, no se extraen del HTML)
- Genera timestamp UTC

**C√≥digo clave**:
```python
def parse(self, html: str, *, currency: str = "USD", region: str = "US") -> ScrapedPrice | None:
    soup = self.parse_html(html)
    raw = self.extract_text(soup.select_one(self.price_selector))
    if not raw:
        return None

    if raw.lower() == "free":
        price_value = Decimal("0.00")
    else:
        cleaned = self._CURRENCY_SYMBOLS.sub("", raw).replace(",", ".")
        try:
            price_value = Decimal(cleaned)
        except InvalidOperation:
            return None  # No se pudo parsear el precio

    return ScrapedPrice(
        price=price_value,
        currency=currency,
        region=region,
        timestamp=datetime.now(UTC),
    )
```

**Decisi√≥n de dise√±o**: `currency` y `region` no se extraen del HTML porque Apple no los expone de forma estructurada. Se asume `USD` y `US` por defecto (la URL hardcodeada es `/us/app`). En el futuro, para scraping multi-regi√≥n, se pasar√°n como par√°metros.

#### 7.4 ReviewParser

Extrae lista de reviews:
- Busca todos los contenedores que coincidan con `container_selector`
- Por cada contenedor extrae: `id`, `rating`, `title`, `content`, `author_name`, `review_date`
- El `rating` se extrae de `aria-label` o del texto usando regex
- La fecha se parsea usando m√∫ltiples formatos de fecha

**C√≥digo clave**:
```python
def parse(self, html: str) -> list[ScrapedReview]:
    soup = self.parse_html(html)
    containers = soup.select(self.container_selector)
    reviews: list[ScrapedReview] = []

    for container in containers:
        review_id = container.get(self.id_attr)
        if not review_id:
            continue  # Skip si no tiene ID

        rating_el = container.select_one(self.rating_selector)
        rating_text = self.extract_attr(rating_el, "aria-label") or self.extract_text(rating_el)
        rating = self._parse_rating(rating_text)  # Extrae d√≠gito con regex
        if rating is None:
            continue  # Skip si no se pudo parsear rating

        date_text = self.extract_text(container.select_one(self.date_selector))
        review_date = self._parse_date(date_text)  # Intenta m√∫ltiples formatos

        reviews.append(ScrapedReview(...))

    return reviews
```

**Decisi√≥n de dise√±o**: Si un review no tiene `review_id` o `rating` v√°lido, se skip (no se agrega a la lista). Los otros campos opcionales se pasan como `None`. Esto evita guardar reviews incompletos que no aportan valor anal√≠tico.

---

### 8. Normalizers ‚Äî Funciones Puras de Transformaci√≥n

**Archivo**: `src/modules/scraping/normalizers.py`

**Responsabilidades**:
- Normalizar moneda: s√≠mbolo ‚Üí c√≥digo ISO 4217 (`$` ‚Üí `USD`)
- Asegurar timezone en datetime: datetime naive ‚Üí datetime aware (UTC)

#### 8.1 normalize_currency

Mapea s√≠mbolos de moneda a c√≥digos ISO 4217:
- Primero busca en `CURRENCY_SYMBOL_MAP`: `$` ‚Üí `USD`, `‚Ç¨` ‚Üí `EUR`
- Si no encuentra, intenta como c√≥digo ISO (case-insensitive): `usd` ‚Üí `USD`
- Si no es v√°lido, lanza `ValueError`

**Decisi√≥n de dise√±o**: Funci√≥n pura sin side effects. Es testeable en aislamiento. Se usa en el validator de `ScrapedPrice.currency`.

**C√≥digos soportados**: USD, EUR, GBP, JPY, KRW, INR, RUB, BRL, TRY, VND, PHP, THB, PLN, SEK, CHF, y 40+ m√°s.

#### 8.2 ensure_timezone_aware

Convierte datetime naive en datetime aware (UTC):
```python
def ensure_timezone_aware(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        return dt.replace(tzinfo=UTC)
    return dt
```

**Decisi√≥n de dise√±o**: Siempre asume UTC para datetime naive. PostgreSQL `timestamp with time zone` requiere timezone. Evita errores de comparaci√≥n entre datetime naive y aware.

---

### 9. Schemas Pydantic ‚Äî Data Quality Firewall

**Archivo**: `src/modules/scraping/schemas.py`

**Responsabilidades**:
- Definir contratos de datos con tipos estrictos
- Validar longitudes m√°ximas (alineadas con columnas de DB)
- Normalizar datos antes de persistir (strip, uppercase, timezone)
- Lanzar `ValidationError` si los datos no cumplen las reglas

**Concepto clave: Data Quality Firewall**

Los schemas Pydantic act√∫an como una barrera de validaci√≥n entre el "mundo sucio" (HTML crudo) y el "mundo limpio" (PostgreSQL). Ning√∫n dato entra a la DB sin pasar por estos schemas.

#### 9.1 ScrapedApp

Contrato para metadata de apps:
- `name` (required): String no vac√≠o, max 255 chars
- `bundle_id` (required): String no vac√≠o, max 255 chars
- `developer_name` (optional): Max 255 chars
- `description` (optional): Sin l√≠mite (TEXT en DB)
- `icon_url` (optional): Max 512 chars

**Validadores**:
```python
@field_validator("name", "bundle_id", mode="before")
@classmethod
def strip_and_require(cls, v: str, info) -> str:
    stripped = v.strip()
    if not stripped:
        raise ValueError(f"{info.field_name} cannot be empty")
    return stripped

@field_validator("name", mode="after")
@classmethod
def name_max_length(cls, v: str) -> str:
    if len(v) > 255:
        raise ValueError("name exceeds max length of 255")
    return v
```

**Decisi√≥n de dise√±o**: Validators en modo `before` transforman el input (strip). Validators en modo `after` validan el resultado. Si falla la validaci√≥n, Pydantic lanza `ValidationError` que se captura en el scraper.

#### 9.2 ScrapedPrice

Contrato para precios:
- `price`: Decimal >= 0, max 99999999.99
- `currency`: String de 3 chars, normalizado a ISO 4217
- `region`: String de max 5 chars, uppercase
- `timestamp`: Datetime con timezone

**Validadores clave**:
```python
@field_validator("currency", mode="before")
@classmethod
def currency_normalize(cls, v: str) -> str:
    return normalize_currency(v)  # Llama al normalizer

@field_validator("timestamp", mode="after")
@classmethod
def timestamp_tz(cls, v: datetime) -> datetime:
    return ensure_timezone_aware(v)  # Asegura timezone UTC
```

**Decisi√≥n de dise√±o**: La normalizaci√≥n de moneda se hace autom√°ticamente en el validator. El parser puede pasar `"$"` y el schema lo convierte a `"USD"`. Separaci√≥n de responsabilidades: parser extrae, schema normaliza.

#### 9.3 ScrapedReview

Contrato para reviews:
- `external_review_id` (required): String no vac√≠o, max 255 chars
- `rating` (required): Entero entre 1 y 5
- `title` (optional): Max 500 chars
- `content` (optional): Sin l√≠mite (TEXT en DB)
- `author_name` (optional): Max 255 chars
- `review_date` (optional): Datetime con timezone

**Validadores**:
```python
@field_validator("rating", mode="after")
@classmethod
def rating_range(cls, v: int) -> int:
    if v < 1 or v > 5:
        raise ValueError("Rating must be between 1 and 5")
    return v
```

**Decisi√≥n de dise√±o**: Si el parser extrae un rating=0 o rating=6, el validator lo rechaza. Esto evita data corruption en la DB (rating fuera de rango).

#### 9.4 ScrapeResult

Contenedor de resultado final:
- `url`: URL scrapeada (para logging)
- `app`: ScrapedApp | None
- `price`: ScrapedPrice | None
- `reviews`: Lista de ScrapedReview (vac√≠a por defecto)
- `success`: Bool (True si no hubo excepciones)
- `error`: String de error (None si success=True)

**Decisi√≥n de dise√±o**: `ScrapeResult` permite resultados parciales. Puede tener `app` pero no `price` (ej. app gratuita sin selector de precio). O puede tener `success=False` con `error` explicando qu√© fall√≥.

---

### 10. Modelos SQLAlchemy ‚Äî Persistencia de Datos

**Archivo**: `src/modules/apps/models.py`

**Responsabilidades**:
- Definir tablas de PostgreSQL usando SQLAlchemy 2.0 (async)
- Mapear tipos de Python a tipos de PostgreSQL
- Definir relaciones y constraints (FK, unique, √≠ndices)

#### 10.1 App

Tabla principal de apps:
- `id`: UUID primary key
- `name`: String(255) NOT NULL
- `bundle_id`: String(255) NOT NULL
- `store`: Enum (APPLE_APP_STORE, GOOGLE_PLAY_STORE) NOT NULL
- `developer_name`: String(255) NULL
- `description`: TEXT NULL
- `icon_url`: String(512) NULL
- `is_active`: Boolean default True
- `created_at`, `updated_at`: Timestamps autom√°ticos (TimestampMixin)

**Constraints**:
- Unique: `(bundle_id, store)` ‚Äî No duplicar apps por store

**Decisi√≥n de dise√±o**: El `bundle_id` NO es √∫nico global. Puede haber un `com.example.app` en Apple y otro en Google. La combinaci√≥n `(bundle_id, store)` es la clave de negocio.

#### 10.2 PriceHistory

Tabla particionada por timestamp (RANGE):
- `id`: UUID (parte de PK compuesta)
- `timestamp`: Timestamp with timezone (parte de PK, partition key)
- `app_id`: UUID (NO es FK, integridad en aplicaci√≥n)
- `price`: Numeric(10,2) NOT NULL
- `currency`: String(3) NOT NULL
- `region`: String(5) NOT NULL

**Constraints**:
- Primary Key: `(id, timestamp)` ‚Äî Requerido por particionamiento
- No foreign key: PostgreSQL no soporta FK en tablas particionadas

**Decisi√≥n de dise√±o**: La partici√≥n por timestamp permite queries eficientes de series temporales: "precios de la √∫ltima semana". Se crean particiones mensuales (configuradas en Alembic). La integridad referencial con `apps.id` se valida en la aplicaci√≥n (el worker verifica que `app_id` exista antes de insertar).

#### 10.3 Review

Tabla de reviews:
- `id`: UUID primary key
- `app_id`: UUID FK ‚Üí apps.id ON DELETE CASCADE
- `external_review_id`: String(255) NOT NULL
- `rating`: Integer NOT NULL
- `title`: String(500) NULL
- `content`: TEXT NULL
- `author_name`: String(255) NULL
- `review_date`: Timestamp with timezone NULL
- `metadata_`: JSONB default {}
- `created_at`, `updated_at`: Timestamps autom√°ticos

**Constraints**:
- Unique: `(app_id, external_review_id)` ‚Äî No duplicar reviews
- Index: `(app_id, review_date)` ‚Äî Queries por app y rango de fechas
- GIN Index: `metadata_` ‚Äî B√∫squedas en JSONB

**Decisi√≥n de dise√±o**: El `external_review_id` es el ID que usa el store (ej. atributo `data-review-id` de Apple). La unique constraint evita insertar el mismo review dos veces si se re-scrapea la app. El `metadata_` permite almacenar campos custom sin modificar el schema (ej. "helpful_count", "language").

---

### 11. Persistencia ‚Äî Funci√≥n _save_scrape_result

**Ubicaci√≥n**: `src/worker/tasks.py`

**Responsabilidades**:
- Recibir `ScrapeResult` validado
- Actualizar metadata de `App` si cambi√≥
- Insertar registro en `PriceHistory`
- Insertar reviews nuevas (skip si ya existen)
- Hacer commit de la transacci√≥n

**Flujo de persistencia**:

1. **Actualizar App**:
```python
if result.app:
    app.name = result.app.name  # Siempre actualiza nombre
    if result.app.developer_name:
        app.developer_name = result.app.developer_name
    if result.app.description:
        app.description = result.app.description
    if result.app.icon_url:
        app.icon_url = result.app.icon_url
```

2. **Insertar PriceHistory**:
```python
if result.price:
    price = PriceHistory(
        app_id=app.id,
        price=result.price.price,
        currency=result.price.currency,
        region=result.price.region,
        timestamp=result.price.timestamp,
    )
    session.add(price)
```

3. **Insertar Reviews (deduplicaci√≥n)**:
```python
for review_data in result.reviews:
    existing = await session.execute(
        select(Review).where(
            Review.app_id == app.id,
            Review.external_review_id == review_data.external_review_id,
        )
    )
    if existing.scalar_one_or_none() is None:
        review = Review(
            app_id=app.id,
            external_review_id=review_data.external_review_id,
            rating=review_data.rating,
            title=review_data.title,
            content=review_data.content,
            author_name=review_data.author_name,
            review_date=review_data.review_date,
        )
        session.add(review)
```

4. **Commit**:
```python
await session.commit()
```

**Decisi√≥n de dise√±o**: La funci√≥n hace un SELECT por cada review para verificar si existe. Esto es ineficiente para lotes grandes (N+1 query problem). Alternativa futura: bulk insert con `ON CONFLICT DO NOTHING` usando `insert().on_conflict_do_nothing()`. Por ahora, la simplicidad es m√°s importante (cada scrape trae ~10 reviews, no 10,000).

---

## üõ°Ô∏è Data Quality Firewall ‚Äî Normalizaci√≥n y Validaci√≥n

El **Data Quality Firewall** es el conjunto de capas defensivas que garantizan que solo datos limpios, normalizados y v√°lidos entren a la base de datos. Est√° compuesto por:

### Capa 1: Normalizers (Transformaci√≥n)

**Ubicaci√≥n**: `src/modules/scraping/normalizers.py`

**Funciones puras** que transforman datos crudos en formatos est√°ndar:
- `normalize_currency(raw: str) -> str`: `$` ‚Üí `USD`, `‚Ç¨` ‚Üí `EUR`
- `ensure_timezone_aware(dt: datetime) -> datetime`: naive ‚Üí UTC

**Caracter√≠sticas**:
- Sin side effects (idempotentes)
- Testeable en aislamiento
- Lanza `ValueError` si el input es inv√°lido

### Capa 2: Pydantic Validators (Validaci√≥n)

**Ubicaci√≥n**: `src/modules/scraping/schemas.py`

**Field validators** que validan tipos, rangos y longitudes:
- `strip_and_require`: No permite strings vac√≠os en campos requeridos
- `name_max_length`: No permite nombres > 255 chars (columna DB)
- `price_range`: No permite precios negativos o > 99999999.99
- `rating_range`: No permite ratings fuera de 1-5
- `currency_normalize`: Llama al normalizer y valida que sea ISO 4217 v√°lido
- `timestamp_tz`: Asegura que todas las fechas tengan timezone

**Caracter√≠sticas**:
- Ejecutan autom√°ticamente al instanciar un schema
- Lanzan `ValidationError` con mensaje descriptivo
- Se ejecutan en orden: `before` ‚Üí coerci√≥n de tipo ‚Üí `after`

### Capa 3: Database Constraints (√öltima L√≠nea de Defensa)

**Ubicaci√≥n**: `src/modules/apps/models.py`

**Constraints de PostgreSQL** que previenen data corruption:
- `NOT NULL`: No permite valores nulos en campos requeridos
- `UNIQUE`: No permite duplicados en `(bundle_id, store)` o `(app_id, external_review_id)`
- `CHECK` (no usado a√∫n): Podr√≠a agregar `CHECK (rating >= 1 AND rating <= 5)`
- `Foreign Key`: Asegura que `app_id` exista en `apps` (solo en tabla `reviews`, no en `price_history`)

**Caracter√≠sticas**:
- √öltima l√≠nea de defensa si los validators fallan
- Genera excepciones de integridad (`IntegrityError`) que el worker captura

### Flujo del Data Quality Firewall

```
HTML crudo
    ‚Üì
[Parser] ‚Üí Extrae strings, n√∫meros, fechas crudas
    ‚Üì
[Pydantic Schema] ‚Üí Instancia ScrapedApp, ScrapedPrice, ScrapedReview
    ‚Üì
[Validators (before)] ‚Üí Llama normalizers, transforma datos
    ‚Üì
[Coerci√≥n de tipos] ‚Üí Convierte str ‚Üí Decimal, str ‚Üí datetime
    ‚Üì
[Validators (after)] ‚Üí Valida rangos, longitudes, formatos
    ‚Üì
[Objeto validado] ‚Üí ScrapedApp, ScrapedPrice, ScrapedReview tipados y limpios
    ‚Üì
[SQLAlchemy Model] ‚Üí Mapea a App, PriceHistory, Review
    ‚Üì
[PostgreSQL] ‚Üí INSERT con constraints de integridad
    ‚Üì
Datos persistidos ‚úÖ
```

**Si falla alguna capa**:
- Pydantic lanza `ValidationError` ‚Üí Scraper retorna `ScrapeResult(success=False)`
- PostgreSQL lanza `IntegrityError` ‚Üí Worker hace rollback y loggea error

---

## üéØ Decisiones de Dise√±o Relevantes

### 1. ¬øPor qu√© Arq y no Celery?

**Arq** es async-first, dise√±ado para `asyncio`. **Celery** es sync-first (usa threading/multiprocessing).

**Market Sentinel es 100% async** (FastAPI, SQLAlchemy async, httpx). Mezclar sync y async complica el c√≥digo (hay que usar `asyncio.run()` o `run_in_executor()`).

**Ventajas de Arq**:
- Sintaxis simple: funciones async normales
- Configuraci√≥n minimalista (una clase `WorkerSettings`)
- Integraci√≥n directa con Redis (no necesita brokers extra)

**Desventaja de Arq**:
- Menos features que Celery (no tiene canvas, chains, grupos complejos)
- Comunidad m√°s peque√±a

**Decisi√≥n**: Arq es suficiente para MVP. Si se necesitan workflows complejos (ej. "scrapear app, despu√©s analizar sentimiento, despu√©s enviar email"), se puede migrar a Temporal o Prefect.

### 2. ¬øPor qu√© no usar ORM para batch inserts?

La funci√≥n `_save_scrape_result` hace un SELECT por cada review para verificar si existe. Esto es un **N+1 query problem**.

**Alternativa**: Usar `INSERT ... ON CONFLICT DO NOTHING`:
```python
stmt = insert(Review).values([...]).on_conflict_do_nothing(
    index_elements=['app_id', 'external_review_id']
)
await session.execute(stmt)
```

**Por qu√© no se usa ahora**:
- Simplicidad: El c√≥digo actual es f√°cil de entender
- Escala suficiente: Cada scrape trae ~10 reviews, no 10,000
- Premature optimization: No hay evidencia de que sea un bottleneck

**Cu√°ndo migrar**: Cuando se implemente scraping masivo (1000+ apps/hora) o scraping de reviews paginadas (100+ reviews por app).

### 3. ¬øPor qu√© Decimal en vez de float para precios?

**float** tiene errores de precisi√≥n: `0.1 + 0.2 = 0.30000000000000004`

**Decimal** tiene precisi√≥n exacta: `Decimal("0.1") + Decimal("0.2") = Decimal("0.3")`

**Regla de oro**: Usar `Decimal` para dinero, cantidades financieras, porcentajes. Usar `float` para c√°lculos cient√≠ficos donde la precisi√≥n aproximada es suficiente.

PostgreSQL `NUMERIC(10,2)` es equivalente a `Decimal` de Python.

### 4. ¬øPor qu√© timestamp con timezone?

**Datetime naive** (`2024-01-15 14:30:00`) no tiene informaci√≥n de zona horaria. Es ambiguo.

**Datetime aware** (`2024-01-15 14:30:00+00:00`) tiene zona horaria (UTC). Es inequ√≠voco.

**PostgreSQL** distingue:
- `timestamp without time zone`: Almacena solo fecha y hora (naive)
- `timestamp with time zone`: Almacena fecha, hora y offset (aware)

**Decisi√≥n**: Siempre usar `timestamp with time zone` y almacenar en UTC. Al leer, convertir a la zona horaria del usuario si es necesario.

### 5. ¬øPor qu√© particionamiento en PriceHistory?

**Problema**: La tabla `price_history` crecer√° indefinidamente. Si scrapeamos 1000 apps cada hora, en un a√±o tendremos 8.7M registros.

**Queries t√≠picos**: "Precios de los √∫ltimos 7 d√≠as", "Evoluci√≥n de precio de una app en el √∫ltimo mes".

**Soluci√≥n**: Particionar por `timestamp` (RANGE). Cada partici√≥n es una tabla separada:
- `price_history_2024_01` (enero 2024)
- `price_history_2024_02` (febrero 2024)

**Ventajas**:
- Queries de rango solo escanean particiones relevantes (partition pruning)
- Drop de particiones antiguas es instant√°neo (no necesita DELETE masivo)
- Archivado f√°cil: mover particiones viejas a S3

**Desventaja**:
- Complejidad operacional: hay que crear particiones nuevas cada mes
- No soporta FK (integridad se valida en aplicaci√≥n)

### 6. ¬øPor qu√© HTTPClient usa fake-useragent?

**Problema**: Algunos app stores detectan bots por User-Agent est√°tico (ej. `python-httpx/0.25.0`) y bloquean requests (HTTP 403).

**Soluci√≥n**: Rotar User-Agent usando `fake-useragent`, que devuelve UAs de browsers reales (Chrome, Firefox, Safari).

**C√≥digo**:
```python
self._ua = UserAgent(browsers=["Chrome", "Firefox", "Safari"])
headers = {"User-Agent": self._ua.random}
```

**Ventaja**: Cada request tiene un UA distinto, parece tr√°fico de usuarios reales.

**Desventaja**: No es suficiente contra anti-bot avanzados (ej. Cloudflare, PerimeterX). Para esos casos, necesitamos browser automation (Playwright).

### 7. ¬øPor qu√© BaseScraper.scrape_batch usa sem√°foro?

**Problema**: `asyncio.gather()` lanza todos los coroutines en paralelo. Si hay 1000 apps, hace 1000 requests simult√°neos, saturando conexiones TCP y disparando rate limits.

**Soluci√≥n**: Usar `asyncio.Semaphore(limit)` para controlar concurrencia:
```python
semaphore = asyncio.Semaphore(10)  # Max 10 requests concurrentes
async with semaphore:
    return await scraper.scrape(bundle_id)
```

**Ventaja**: El sistema se comporta como un rate limiter suave (no m√°s de 10 requests/s).

**Configuraci√≥n**: `scraper_concurrency` en `Settings` (default: 5).

### 8. ¬øPor qu√© los parsers son instancias y no funciones?

**Alternativa**: Definir parsers como funciones:
```python
def parse_app_metadata(html: str, bundle_id: str) -> ScrapedApp | None:
    soup = BeautifulSoup(html, "html.parser")
    name = soup.select_one("h1.product-header__title").get_text(strip=True)
    ...
```

**Problema**: Los CSS selectors est√°n hardcodeados en el body. Si cambio el selector, tengo que modificar la funci√≥n.

**Soluci√≥n**: Parsers como clases con selectores configurables:
```python
parser = AppMetadataParser(
    name_selector="h1.product-header__title",
    developer_selector="h2.product-header__identity a",
)
app = parser.parse(html, bundle_id)
```

**Ventaja**:
- Testeable: Puedo inyectar selectores fake en tests
- Reutilizable: El mismo parser funciona para Apple y Google (con selectores distintos)
- Extensible: Puedo crear subclases que sobrescriben `parse()`

**Desventaja**: M√°s verboso (necesita `__init__`).

---

## üìä M√©tricas y Observabilidad

Todos los componentes usan **structlog** para logging estructurado (JSON).

**Logs clave**:

### HTTPClient
- `http_request_start` ‚Äî Request iniciado (url, user_agent)
- `http_request_ok` ‚Äî Request exitoso (url, status_code)
- `http_rate_limited` ‚Äî Rate limit detectado (url, retry_after)

### Scraper
- `apple_scrape_ok` ‚Äî Scraping exitoso (url, has_app, has_price, review_count)
- `apple_scrape_error` ‚Äî Scraping fallido (url, error)
- `scrape_batch_exception` ‚Äî Excepci√≥n en batch (bundle_id, error)

### Worker
- `worker_startup` ‚Äî Worker iniciado
- `worker_shutdown` ‚Äî Worker apagado
- `scrape_app_task_start` ‚Äî Tarea iniciada (app_id)
- `scrape_app_task_done` ‚Äî Tarea completada (app_id, success, error)
- `scrape_app_not_found` ‚Äî App no existe en DB (app_id)

**Uso**:
```python
logger.info("apple_scrape_ok", url=url, has_app=True, review_count=12)
# Output JSON:
# {"event": "apple_scrape_ok", "url": "https://...", "has_app": true, "review_count": 12, "timestamp": "2024-01-15T14:30:00Z"}
```

**Ventaja de JSON logs**: Se pueden ingerir en ELK, Datadog, CloudWatch y hacer queries: "¬øCu√°ntos scrapes fallaron en la √∫ltima hora?" ‚Üí `event:apple_scrape_error AND timestamp:[now-1h TO now]`

---

## üöÄ Flujo de Datos Completo ‚Äî Ejemplo Real

### Escenario: Usuario solicita scraping de Instagram (Apple App Store)

1. **Cliente HTTP**:
```bash
curl -X POST http://localhost:8000/scrape/abc123-def456-789
```

2. **API** (`src/api/__init__.py`):
```python
app_id = "abc123-def456-789"
pool = await _get_arq_pool()  # Conexi√≥n a Redis
job = await pool.enqueue_job("scrape_app_task", app_id)
# Response: {"job_id": "xyz789"}
```

3. **Redis**: Job encolado en `arq:queue`

4. **Worker** (`src/worker/tasks.py`):
```python
async def scrape_app_task(ctx, app_id):
    # SELECT * FROM apps WHERE id = 'abc123-def456-789'
    app = await session.execute(select(App).where(App.id == uuid.UUID(app_id)))
    # Result: App(bundle_id="389801252", store=APPLE_APP_STORE)
```

5. **HTTPClient** (`src/modules/scraping/client.py`):
```python
url = "https://apps.apple.com/us/app/id389801252"
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
response = await httpx.get(url, headers=headers)
# Response: 200 OK, HTML de 500 KB
```

6. **AppleStoreScraper** (`src/modules/scraping/stores/apple.py`):
```python
html = response.text
app = _APP_METADATA_PARSER.parse(html, "389801252")
# ScrapedApp(name="Instagram", bundle_id="389801252", developer_name="Instagram, Inc.", ...)
price = _PRICE_PARSER.parse(html)
# ScrapedPrice(price=Decimal("0.00"), currency="USD", region="US", timestamp=datetime(...))
reviews = _REVIEW_PARSER.parse(html)
# [ScrapedReview(...), ScrapedReview(...), ...]
```

7. **Pydantic Validation** (`src/modules/scraping/schemas.py`):
```python
# Valida ScrapedApp:
# - name="Instagram" ‚úÖ (no vac√≠o, <= 255 chars)
# - bundle_id="389801252" ‚úÖ
# - developer_name="Instagram, Inc." ‚úÖ
# Valida ScrapedPrice:
# - price=0.00 ‚úÖ (>= 0, <= 99999999.99)
# - currency="USD" ‚úÖ (ISO 4217 v√°lido)
# - timestamp con timezone ‚úÖ
# Valida ScrapedReview:
# - rating=5 ‚úÖ (entre 1 y 5)
```

8. **Persistencia** (`src/worker/tasks.py`):
```python
# UPDATE apps SET name='Instagram', developer_name='Instagram, Inc.' WHERE id='abc123...'
app.name = "Instagram"
app.developer_name = "Instagram, Inc."

# INSERT INTO price_history (app_id, price, currency, region, timestamp) VALUES (...)
session.add(PriceHistory(app_id=app.id, price=0.00, currency="USD", region="US", timestamp=...))

# Para cada review:
# SELECT * FROM reviews WHERE app_id='abc123...' AND external_review_id='review_001'
# Si no existe:
#   INSERT INTO reviews (app_id, external_review_id, rating, title, ...) VALUES (...)
for review_data in reviews:
    if not exists:
        session.add(Review(app_id=app.id, external_review_id=..., rating=5, ...))

# COMMIT
await session.commit()
```

9. **Worker** completa el job:
```python
logger.info("scrape_app_task_done", app_id=app_id, success=True)
return {"success": True, "error": None}
```

10. **Redis**: Job marcado como completado

---

## üîÆ Extensibilidad Futura

### 1. Scraping Multi-Regi√≥n

**Problema**: Ahora solo scrapeamos `us` (Estados Unidos). Apps tienen precios distintos por regi√≥n.

**Soluci√≥n**:
- Modificar `AppleStoreScraper.build_url()` para aceptar par√°metro `region`:
```python
def build_url(self, bundle_id: str, *, region: str = "us") -> str:
    return f"https://apps.apple.com/{region}/app/id{bundle_id}"
```
- Pasar lista de regiones en `scrape_app_task`:
```python
regions = ["us", "mx", "br", "de", "jp"]
for region in regions:
    url = scraper.build_url(bundle_id, region=region)
    scrape_result = await scraper.scrape_url(url, region=region)
```

### 2. Scraping de Reviews Paginadas

**Problema**: Ahora solo scrapeamos la primera p√°gina (~10 reviews). Apps populares tienen miles de reviews.

**Soluci√≥n**:
- Agregar m√©todo `scrape_reviews_paginated()` que sigue links de paginaci√≥n:
```python
async def scrape_reviews_paginated(self, bundle_id: str, *, max_pages: int = 10) -> list[ScrapedReview]:
    reviews = []
    url = self.build_url(bundle_id)
    for page in range(max_pages):
        html = await self.client.get(url)
        page_reviews = _REVIEW_PARSER.parse(html)
        reviews.extend(page_reviews)
        next_url = self._extract_next_page_url(html)
        if not next_url:
            break
        url = next_url
    return reviews
```

### 3. Browser Automation para Stores con JavaScript

**Problema**: Algunos stores renderizan contenido din√°mico con JavaScript (ej. Google Play usa React). `httpx` solo obtiene HTML est√°tico.

**Soluci√≥n**:
- Usar Playwright para scrapear con browser headless:
```python
from playwright.async_api import async_playwright

async def scrape_with_browser(url: str) -> str:
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url)
        html = await page.content()
        await browser.close()
        return html
```

### 4. Scraping Incremental (Delta Updates)

**Problema**: Ahora re-scrapeamos toda la app cada vez. Si solo cambi√≥ el precio, desperdiciamos CPU parseando metadata.

**Soluci√≥n**:
- Agregar campo `last_scraped_at` en tabla `apps`
- Comparar timestamp: si `last_scraped_at` < 1 hora, solo scrapear precio
- Implementar parsers incrementales que reciben `fields_to_parse=["price"]`

### 5. Retry con Backoff Exponencial en Worker

**Problema**: Si el scraping falla (HTTP 500, timeout), el job se marca como fallido y no se reintenta.

**Soluci√≥n**:
- Configurar `max_tries` en Arq:
```python
class WorkerSettings:
    functions = [scrape_app_task]
    max_tries = 3  # Reintentar hasta 3 veces
    retry_jobs = True
```

---

## ‚úÖ Checklist de Calidad de C√≥digo

- [x] Tipos est√°ticos en todos los componentes (`Mapped`, type hints)
- [x] Validaci√≥n de datos con Pydantic
- [x] Normalizaci√≥n de datos con funciones puras
- [x] Logging estructurado con structlog
- [x] Retry autom√°tico en HTTPClient
- [x] Rate limiting con sem√°foro
- [x] Deduplicaci√≥n de reviews (unique constraint)
- [x] Particionamiento de tabla de precios
- [x] Context managers para recursos (`async with HTTPClient()`)
- [x] Manejo de excepciones con ScrapeResult
- [x] Tests unitarios de parsers (no incluidos en este an√°lisis, pero existen)
- [x] Separaci√≥n de responsabilidades (parser ‚Üí schema ‚Üí model ‚Üí DB)

---

## üìö Glosario de T√©rminos

**Scraper**: Componente que extrae datos de una fuente externa (web page, API).

**Parser**: Componente que convierte HTML crudo en objetos estructurados.

**Normalizer**: Funci√≥n pura que transforma datos a un formato est√°ndar (ej. `$` ‚Üí `USD`).

**Validator**: Funci√≥n que verifica que los datos cumplan reglas de negocio (ej. rating entre 1-5).

**Schema**: Contrato de datos definido con Pydantic que combina validaci√≥n y normalizaci√≥n.

**Data Quality Firewall**: Conjunto de capas defensivas (normalizers + validators + constraints) que previenen data corruption.

**Producer-Consumer Pattern**: Arquitectura donde un componente (productor) genera trabajos que otro componente (consumidor) procesa as√≠ncronamente.

**Message Broker**: Sistema intermedio (Redis, RabbitMQ) que almacena mensajes entre productor y consumidor.

**Async-first**: Arquitectura donde todo el I/O (DB, HTTP, file system) usa `asyncio` en vez de threading/multiprocessing.

**Partitioning**: Dividir una tabla grande en m√∫ltiples tablas f√≠sicas (particiones) seg√∫n un criterio (ej. rango de fechas).

**Idempotent**: Operaci√≥n que produce el mismo resultado si se ejecuta 1 vez o N veces (ej. `INSERT ... ON CONFLICT DO NOTHING`).

**N+1 Query Problem**: Antipatr√≥n donde se hacen N queries en un loop en vez de 1 query con JOIN.

**Context Manager**: Patr√≥n de Python (`with`, `async with`) que garantiza limpieza de recursos (ej. cerrar conexiones).

---

## üéì Conclusi√≥n

El m√≥dulo de scraping de Market Sentinel implementa una arquitectura robusta y escalable para ingesta de datos de app stores. Los pilares del dise√±o son:

1. **Desacoplamiento**: API y worker separados con Redis como broker
2. **Resiliencia**: Retry autom√°tico, rate limiting, manejo de excepciones
3. **Calidad de datos**: Data Quality Firewall con normalizaci√≥n y validaci√≥n
4. **Observabilidad**: Logging estructurado en todos los componentes
5. **Extensibilidad**: Abstracciones (BaseScraper, parsers configurables) que facilitan agregar stores nuevas

El flujo completo (HTTP request ‚Üí parsing ‚Üí validaci√≥n ‚Üí persistencia) toma ~5 segundos por app. Con 10 workers concurrentes, el sistema puede procesar ~7200 apps/hora.

**Pr√≥ximos pasos**:
- Implementar GooglePlayScraper
- Agregar scraping de reviews paginadas
- Crear dashboard de m√©tricas (success rate, latency, error rate)
- Implementar scraping multi-regi√≥n
- Agregar tests de integraci√≥n end-to-end
